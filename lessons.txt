
Вечерняя школа
#1
#2 ПОДЫ
https://github.com/slurm-personal/school-dev-k8s     //непонятный гит с лекциями от вечерней школы

kubectl get pod
kubectl get po //тоже самое

kubectl create -f pod.yaml (запуск ямла, ямл образец в соседнем файле)
kubectl describe po my-pod //описание пода

kubectl delete po my-pod
kubectl delete -f pod.yaml

#3 REPLICASET

kubectl delete po --all //удалит все поды
kubectl delete all --all -A //снесет все обьекты со всех нэймспэйсов //это опасно
kubectl delete replicaset --all

kubectl create -f replicaset.yaml //пременяется только один раз
kubectl apply -f replicaset.yaml  //перечитает файл применит изменения

kubectl get replicasets
kubectl get rs  //то же самое

kubectl get po -l app=my-app //поиск подов по лэйблу

kubectl scale --replicas 3 replicaset my-replicaset  //увеличение реплик вручную

kubectl set image replicaset my-replicaset nginx=nginx:1.12 // обновление образа вручную

kubectl explain <чтото>  // открывает документашку

    DEPLOYMENT
kubectl apply -f deployment.yaml
kubectl get deploy
kubectl set image deployment my-deployment '*=<image>'
kubectl edit deployment my-deployment (выводит временный файл с описанием деплоймента, после его редактирования изменения применяются но не сохраняються в файл)
kubectl rollout undo deployment my-deployment (откатит изменения, он при создании нового rs не удаляет старый а просто скелит поле replicas)
revisionHistoryLimit: 10 (глубина хранения старых репликасетов после обновлений)

kubectl explain deployment.spec.strategy (обьяснит значение этого поля)

strategy (это способ обновления: грохать сразу все старые реплики или постепенно, есть доп инфа видео №3 прим.1:08:00время)

        NAMESPACES
разделение пространства имен
могут ли одни приложения ходить в другие нэймспэийсы и многое другое зависит от политик

        RESOURCES
Limits ограничения по изпользованию приложением памяти и ядер
Requests количество ресурсов, которое резервируется для пода на ноде
QoS Class поле которое указывает важность пода, в случае если ресерсов будет не хватать, от него зависит кто первый съедет с ноды

#4 ПЕРЕМЕННЫЕ ОКРУЖЕНИЯ

kubectl apply -f configmap.yaml (конфигмап надо заапплаить)
kubectl get cm (просмотреть существующие конфигмапы)
kubectl exec -t my-deployment-7bc97c6ff4-ghvph -- env (посмотреть переменные внутри пода)

СЕКРЕТЫ

generic - пароли токены приложений
docker-registry - данные афторизации в docker registry(секретные данные ключи доступа к различным репозиториям типа git email и др.)
tls - TLS сертификаты для Ingress

kubectl create secret generic test --from-literal=test1=asdf --from-literal=dbpassword=1q2wpass123 (создаст секрет)
kubectl get secrets (выведет кол. созданных секретов без их показа)
delete --all делать не надо удаляться ключи доступа к наймспэйсам
Opaque - непрозрачный  generic==Opaque
kubectl get secrets test -o yaml (покажет ключи и значения закодированные в b64)
echo MXEyd3Bhc3MxMjM= | base64 -d (раскодирует секрет)

пересылаем файлы через переменные через конфигмап
kubectl exec -it my-deployment-5f8455c477-6d56t -- env (можно посмотреть секрет если у разработчика не отобрали exec права)
kubectl exec -it my-deployment-5db5bdc77-f4bdn -- busybox sh (можно зайти внутрь пода и просмотреть файлы)
kubectl port-forward my-deployment-5db5bdc77-f4bdn 8080:80 & (так можно постучаться на порт нжинкса) далее введем-->> curl 127.0.0.1:8080
ps ax | grep 8080   далее ->> kill -9 4023

Donward API

все поля передачи переменных заполнил в deployment.yaml

kubectl get po -o wide (более подробная инфа по подам)

#5
ХРАНЕНИЕ ДАННЫХ

HostPath
монтирует папку из ноды в реплику (образец в deployment.yaml)
обычно запрещается использование HostPath политикой безопасности во избежании доступа разработчикам к конф файлам на ноде

EmptyDir
cri создает временный диск и прокидывает его внутрь контейнера. При рестарте приложения данные сохраняются, при удалении удаляются.
kubectl exec -it my-deployment-5cc9b958f4-zw74m -- busybox sh далее->> mount (посмотреть что куда смонтировано)

PC/PVC (PersistentVolumeClaim/PersistentVolume)
Storage class хранит параметры подключения
PersistentVolumeClaim описывает требования к тому
PersistentVolume хранит параметры и статус тома
Разговор о самостоятельном создании диска на желесках или аренды в облаках
PVC делает запрос на 10GB и ему выдают диск не меньше этого размера
После этого диск переходит в состояние bound
Provisioner Create PV нарезает вольюмы согласно требования pvc, чтобы не отдавать диск на 1000Gb поду с запросом на 10Mb
kubectl apply -f pvc.yaml
много интересного, в том числе и увеличение размера диска
короче если надо покупаешь диск пересматриваешь видео

initConteiners
их может быть несколько
можно монтировать те же тома что и в основных контейнерах

ReadWriteMany
cephFS - (система хранения данных)умеет подключать несколько нод к одному диску

ReadWriteOnce
RBD винчестер умеет монтировать диск только на одну ноду
две реплики на одной ноде смогут использовать RBD и иметь доступ к диску

#6 СЕТЕВЫЕ ИНТЕРФЕЙСЫ

PROBES
Liveness Probe контроль за состоянием приложения исполняется постоянно
Readiness Probe пров. готово ли приложение принимать трафик. Исполняется постоянно. В случае неудачи убирается из баллансировки
StartUp Probe пров. запустилось ли приложение. Исполняется при старте.

SERVICES
ClusterIP внутрикластерное взаимодействие (важно совпадение селектора и лэйблов приложений)

kubectl run test --image=amouat/network-utils -it bash (небольшая утилитка для теста сетей)
kubectl exec test -it -- bash
curl my-service (имя сервиса будет ip адресом)
при обращении на ip адрес сервиса он рандомно подкидывает один из ip адресов нод использующих лэйбл указанный в сервисе
my-service.anothernamespace (так будем обращаться в другой нэймспэйс)


NodePort открывает порты на нодах от 30000 до 32767
curl <host_node>:31780 прокинет в приложение, порт берется из команды ->> get service

LoadBalancer работает только в облаках

ExternalName непонятно
ExternalIps на все ноды устанавливает определенный ip по которому можно попасть в поды тоже не понятно

Headless хрень какаято, в основном применяеться со stateful сервисами

INGRESS
типа почти всегда nginx может быть несколько реплик

Аннотации ingress (очень много аннотаций смотреть документашку к аннотациям ингреса)
есть схемы подключения платных и автоматических сертификатов прим. 1:35:00 на видео



7# УСТОРОЙСТВО КУБЕРНЕТЕСА

Etcd
База данных кубернетеса.
распределенная. содержит нечетное количество узлов
хранит в формате key-value

API Server
центральный REST Api кубернетеса
единственный кто общается с Etcd
k get po -n my-namespace -v10 выведет все запросы к аппи
Занимается аутентификацией и авторизацией


Controller-Manager
бинарный файл в контейнере
набор контроллеров
    Node controller - ноды не выдающие о себе информации больше 5 мин. считаются неживыми. Делает запрос для их пометки в апи сервер. Инач.заниматься переносом подов на другие ноды.
    Replicaset controller - смотрит в апи какие есть репликасэты, и на основании этой информации занимается созданием репликасетов
    Endpoints controller - занимается создание сервисов
    Garbage collector
    И другие...


Sheduler
назначает поды на ноды учитывая QoS, Affinity/Anti-affinity, requested resources, priority class

Kubelet
Работает на каждой ноде кластера
На мастер ноде запрещено запускать поды, но это ограничение можно снять
Единственный процесс кто работает не в докере
Отдает команды docker daemon, через его апи
Запускает поды
Выполняет пробы на подах
Отправляет различную информацию в апи о состоянии ноды, ресурсов и др.

Kube-proxy
Занимается обустройством некоторых сетевых абстракций, правил
Смотрит в апи
Не занимается реализацией самих сетей, этим зан.сет.плагины
Занимается сервисами
Стоит на всех нодах
Управляет сетевыми правилами на нодах
Создает правила в ip-table
Последнее время иногда заменяется на цилиум
Раньше дейсвительно переводил весь трафик через себя, сейчас просто занимается конфигурацией


8#
Minikube
аддоны - в одну команду запускать разные штуки типа ингресса
обучение работы с миникубом
minikube addons list - список аддонов
minikube addons enable ingress - запуск аддона

Dashboard - разрабатывается самим кубернетесом, требует установки

9# Oneshot-задачи

JOB
Создает под для выполнения задачи
Перезапускает поды до успешного выполнения задачи или истечения тайм-аутов
activeDeadLineSeconds - временной лимит
backoffLimit - количество перезапусков
k logs jobs/hello
у джобы можно настроить время жизни, после истеч. кот. абстракция удаляется. в какихто версиях кубера прийдется включить дополнительный контроллер из контроллер менеджера
Parallelism - сколько подов в рамках одной джобы могут выполняться одновременно

CronJob
Создает Job по расписанию
k get cj
по умолчанию количество завершеных сохраненных подов будет 3
а завершенных с ошибкой 1
при пропуске по тех. причинам более 100 джобов джоб-контроллер перестает создавать джобы. Это лечится переустановкой джоба

10#
DaemonSet
StatefulSet

Static Pod
способ запуска подов независимый от апи
минус - не управляется с помощью kubectl
придуманы для изначального запуска кластера, при старте kubelet на мастере он поднимает поды остальных компонентов мастера из статик подов

Pod Anti Afinity
распределяет поды по одному на ноду
если нод больше чем заданное число в деплойменте, прийдется поправить деплоймент

Daemon Set
Запускает поды на всех нодах кластера
при добавлении ноды добавляет под, при удалении удаляет
описание почти как у деплоймента, кроме количества реплик
k get nodes --show-labels (покажет ноды со всеми лэйблами)
k get nods -l kubernetes.io/os=linux (выберет ноды с конкретным лэйблом)

    taints: (обьявляется на ноде, типа появилась зараза и все поды эвакуируются с этого узла)
    - effect: NoSchedule
      key: node-role.kubernetes.io/master

    tolerations: (обьявляется на поде, который имеет устойчивость к этой заразе и может быть запущен на ноде с этой заразой)
    - effect: NoSchedule
      operator: Exists
      key: node-role.kubernetes.io/master

k get node 608686.cloud4box.ru -o yaml (просмотр описания ноды)

- effect: NoExecute вешается на те узлы, которые собираются из кластера выводиться